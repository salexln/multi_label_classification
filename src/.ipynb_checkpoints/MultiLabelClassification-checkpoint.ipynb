{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLabel Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies:\n",
    "#### Python (2.7)\n",
    "* pip install sklearn\n",
    "* pip install scikit-multilearn\n",
    "* pip install future\n",
    "* pip install python-igraph\n",
    "* python-graph-tool: Use this tutorial to install it on ubuntu https://zhangkaiyuan.com/2018/03/10/Install-graphtools-on-Ubuntu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* http://scikit.ml/api/classify.html#ensemble-approaches\n",
    "* http://scikit.ml/\n",
    "* https://en.wikipedia.org/wiki/Multi-label_classification\n",
    "* https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n",
    "* https://pdfs.semanticscholar.org/6b56/91db1e3a79af5e3c136d2dd322016a687a0b.pdf\n",
    "* http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/pr07.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "We will generate artificial ,multi-label dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sparse=True: returns sparse matrix (matrix having large amount of zero elemets)\n",
    "n_labels: The average number of labels per instance\n",
    "return_indicator='sparse': returns the sparse binary indicator format.\n",
    "allow_unlabeled: If True, some instances might not belong to any class.\n",
    "\"\"\"\n",
    "X, y = make_multilabel_classification(sparse=True, n_labels=20, return_indicator = 'sparse', allow_unlabeled = False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformation methods:\n",
    "Problem transformation methods transform a multi-label classification problem in one or more single-label classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Binary relevance:\n",
    "This baseline approach, amounts to independently training one binary classifier for each label: Given an unseen sample, the combined model then predicts all labels for this sample for which the respective classifiers predict a positive result.\n",
    "\n",
    "Suppose we have q labels, the binary relevance method creates q new data sets, one for each label and train single-label classifiers on each new data set.\n",
    "(Note that this approach will not work well when there's dependencies between the labels). \n",
    "\n",
    "![title](img/binary_relevance1.png)\n",
    "![title](img/binary_relevance2.png)\n",
    "\n",
    "\n",
    "![title](img/binary_relevance_chart.png)\n",
    "\n",
    "This method is the most simple and efficient it has one drawback: doesnâ€™t consider labels correlation because it treats every target variable independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69999999999999996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "\n",
    "# train:\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict:\n",
    "prediction = classifier.predict(X_test)\n",
    "\n",
    "# check accuracy:\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Classifier chains:\n",
    "In this method, each classifier is trained on the output of the previous classifier (when the first one is trained on the input data)\n",
    "\n",
    "\n",
    "For example, if we have the following dataset X with 4 labels Y:\n",
    "![title](img/classifier_chains2.png)\n",
    "\n",
    "We would transform the probelm to the following:\n",
    "(The white represents the output and the yellow the input)\n",
    "![title](img/classifier_chains.png)\n",
    "\n",
    "We can think of classifier chains in the following way:\n",
    "![title](img/classifier_chains_chart.png)\n",
    "\n",
    "This method combines the computational efficiency of the Binary Relevance  while still being able to take the label dependencies into account for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84999999999999998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "classifier = ClassifierChain(GaussianNB())\n",
    "\n",
    "# train:\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict:\n",
    "prediction = classifier.predict(X_test)\n",
    "\n",
    "# check accuracy:\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Label powerset:\n",
    "In this method, we will transform the problem into a multi-class: this transformation creates one binary classifier for every label combination found in the training set.\n",
    "\n",
    "For example, the follwoing dataset\n",
    "![title](img/label_powerset.png)\n",
    "turn into the follwoing:\n",
    "![title](img/label_powerset2.png)\n",
    "(we can see that (x1, x4) and (x3, x6) have the same labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "classifier = LabelPowerset(GaussianNB())\n",
    "\n",
    "# train:\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict:\n",
    "prediction = classifier.predict(X_test)\n",
    "\n",
    "# check accuracy:\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adapted algorithm\n",
    "Instead of changing the problems, we could change the algorithm to support multi label classification.\n",
    "\n",
    "Example of these algorithms are KNN, desicion trees, boosting, neural networks, ets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multi Lavel KNN (MLkNN)\n",
    "\n",
    "MLkNN is derived from the traditional K-nearest neighbor (KNN) algorithm. \n",
    "In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances (i.e. the number of neighboring instances belonging to each possible class) maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. \n",
    "\n",
    "\n",
    "For more information:\n",
    "http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/pr07.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84999999999999998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "\n",
    "classifier = MLkNN(k=20)\n",
    "\n",
    "# train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "prediction = classifier.predict(X_test)\n",
    "\n",
    "# check accuracy\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ensemble learning\n",
    "Ensemble learning is a machine learning paradigm where multiple learners are trained to solve the same problem.\n",
    "\n",
    "It is often useful to train more than one model for a subset of labels in multi-label classification (for large label spaces - a well-selected smaller label subspace can allow more efficient classification)\n",
    "\n",
    "\n",
    "As rule of thumb, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone\n",
    "\n",
    "An example code for an ensemble of RandomForests under a Label Powerset multi-label classifiers trained for each label subspace - partitioned using fast greedy community detection methods on a label co-occurrence graph looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84999999999999998"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.cluster import IGraphLabelCooccurenceClusterer\n",
    "from skmultilearn.ensemble import LabelSpacePartitioningClassifier\n",
    "\n",
    "# base classifier\n",
    "base_classifier = RandomForestClassifier()\n",
    "\n",
    "# setup problem transformation approach with sparse matrices for random forest\n",
    "problem_transform_classifier = LabelPowerset(classifier=base_classifier,\n",
    "    require_dense=[False, False])\n",
    "\n",
    "# partition the label space using fastgreedy community detection\n",
    "clusterer = IGraphLabelCooccurenceClusterer('fastgreedy', weighted=True, include_self_edges=True)\n",
    "\n",
    "# setup the ensemble metaclassifier\n",
    "classifier = LabelSpacePartitioningClassifier(problem_transform_classifier, clusterer)\n",
    "\n",
    "# train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
